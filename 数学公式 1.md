这段代码实现了一个双层 LSTM（长短时记忆网络）结合多尺度注意力机制（Multi-Scale Attention）和全局池化（Max Pooling 和 Avg Pooling）的深度学习模型，用于时间序列数据的处理和分类任务。它涉及到多个数学原理，覆盖了线性代数、概率论、最优化方法以及深度学习中的经典理论。以下是详细的数学原理介绍：

---

### 1. **LSTM（长短时记忆网络）中的数学原理**
LSTM 是一种特殊的循环神经网络（RNN），它通过引入门控机制（Gates）来解决传统 RNN 中的梯度消失和梯度爆炸问题。以下是其核心数学公式：

#### **门控机制**
LSTM 单元的关键在于三个门控机制：遗忘门、输入门和输出门。
1. **遗忘门**：决定前一时间步的信息是否保留，公式如下：
   \[
   f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)
   \]
   其中 \(f_t\) 是遗忘门的输出，\(\sigma\) 是 Sigmoid 函数，\(W_f\) 和 \(b_f\) 是权重和偏置。

2. **输入门**：决定当前时间步的新信息是否写入，公式如下：
   \[
   i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)
   \]
   \[
   \tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)
   \]
   其中 \(i_t\) 是输入门的输出，\(\tilde{C}_t\) 是候选记忆。

3. **输出门**：决定最终输出的隐藏状态，公式如下：
   \[
   o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)
   \]
   \[
   h_t = o_t \cdot \tanh(C_t)
   \]

#### **记忆状态更新**
LSTM 的核心是记忆单元（Cell State），它通过遗忘门和输入门的共同作用更新：
\[
C_t = f_t \cdot C_{t-1} + i_t \cdot \tilde{C}_t
\]
这一公式确保了信息可以长期存储并动态更新。

---

### 2. **多尺度卷积（Multi-Scale Convolution）**
代码中通过不同大小的卷积核（如 kernel=3、5、7）提取多尺度特征。这是基于卷积操作的数学原理：
\[
y[i] = \sum_{k=0}^{K-1} w[k] \cdot x[i+k]
\]
其中 \(x\) 是输入信号，\(w\) 是卷积核，\(K\) 是卷积核的大小。

多尺度卷积的数学意义在于：
- 小卷积核（如 kernel=3）聚焦于局部特征。
- 大卷积核（如 kernel=7）捕捉更大范围的上下文信息。

这些特征会在后续层中融合，形成多尺度的特征表示。

---

### 3. **自注意力机制（Self-Attention）**
自注意力机制用于捕捉序列中每个时间步之间的全局依赖关系。其核心数学原理包括：
1. **Query-Key-Value 机制**：
   - Query (\(Q\))、Key (\(K\)) 和 Value (\(V\)) 是通过线性变换获得的特征向量：
     \[
     Q = W_q \cdot X, \quad K = W_k \cdot X, \quad V = W_v \cdot X
     \]
     其中 \(W_q, W_k, W_v\) 是权重矩阵。

2. **注意力分数**：
   - 通过 Query 和 Key 的点积计算注意力分数：
     \[
     \text{Score}(Q, K) = \frac{Q \cdot K^T}{\sqrt{d_k}}
     \]
     其中 \(d_k\) 是 Key 的维度，用于缩放分数，防止数值过大。

3. **注意力权重**：
   - 对分数进行 Softmax 归一化，得到权重：
     \[
     \alpha = \text{Softmax}(\text{Score}(Q, K))
     \]

4. **上下文向量**：
   - 利用注意力权重加权求和 Value，得到新的上下文向量：
     \[
     \text{Context} = \alpha \cdot V
     \]

自注意力机制的数学本质在于通过点积操作计算特征向量之间的相似性，从而为序列中的每个时间步分配不同的重要性权重。

---

### 4. **全局池化（Max Pooling 和 Avg Pooling）**
全局池化是对时间序列特征进行特定统计操作的过程，其数学定义如下：
1. **最大池化（Max Pooling）**：
   - 取时间序列中每个特征维度的最大值：
     \[
     y_j = \max(x_{1j}, x_{2j}, \dots, x_{Tj})
     \]
     其中 \(x_{tj}\) 表示第 \(t\) 个时间步的第 \(j\) 个特征。

2. **平均池化（Avg Pooling）**：
   - 取时间序列中每个特征维度的平均值：
     \[
     y_j = \frac{1}{T} \sum_{t=1}^T x_{tj}
     \]
     最大池化用于捕捉显著性特征，而平均池化则反映整体趋势。

通过将两者拼接，模型能够同时捕捉局部突变模式和全局趋势特征。

---

### 5. **残差连接（Residual Connection）**
残差连接是一种缓解深度网络梯度消失问题的技术，其数学形式为：
\[
y = F(x) + x
\]
其中 \(F(x)\) 是网络的非线性变换，\(x\) 是输入。

残差连接的核心思路是直接将输入信息传递到后续层，确保信息的有效保留。这在 LSTM 层间尤为重要，因为时间序列数据可能包含缓慢变化的趋势，而这些趋势可能在深层网络中被削弱。

---

### 6. **正则化与激活函数**
代码中使用了多种正则化和激活技术：
1. **Batch Normalization**：
   - 对每一批数据进行归一化处理，减小数据分布的波动：
     \[
     \hat{x} = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}}
     \]
     其中 \(\mu\) 和 \(\sigma^2\) 是小批量数据的均值和方差。

2. **Dropout**：
   - 随机丢弃部分神经元：
     \[
     y = \begin{cases} 
     0 & \text{with probability } p \\
     x & \text{with probability } 1-p
     \end{cases}
     \]

3. **GELU 激活函数**：
   - GELU 是一种平滑激活函数，其数学表达为：
     \[
     \text{GELU}(x) = x \cdot \Phi(x)
     \]
     其中 \(\Phi(x)\) 是标准正态分布的累积分布函数。

---

### 7. **优化与训练**
代码中使用了 AdamW 优化器和余弦退火学习率调度器，这些方法在深度学习中具有广泛的应用：
1. **AdamW 优化器**：
   - 结合动量和自适应学习率的优化算法，更新规则为：
     \[
     m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t
     \]
     \[
     v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2
     \]
     \[
     \theta_t = \theta_{t-1} - \eta \cdot \frac{m_t}{\sqrt{v_t} + \epsilon}
     \]

2. **余弦退火学习率调度**：
   - 学习率随时间步变化：
     \[
     \eta_t = \eta_{\min} + \frac{1}{2} (\eta_{\max} - \eta_{\min}) \left(1 + \cos\left(\frac{T_{\text{cur}}}{T} \pi\right)\right)
     \]
     其中 \(T_{\text{cur}}\) 是当前时间步，\(T\) 是总时间步。

---

### 总结
这段代码中的数学原理涵盖了深度学习中多个关键领域：LSTM 中的门控机制、自注意力机制中的向量相似性计算、全局池化的统计特征提取，以及残差连接和正则化方法的应用。这些数学基础共同构成了一个强大的时间序列处理模型，使其能够高效捕捉特征并进行精确分类。
